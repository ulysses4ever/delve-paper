\section{Introduction}

The current approach to building data analysis applications is 
dominated by languages like Python, R, and, to some extent, Julia.
All of these languages can be considered as general-purpose; e.g. it is easy
to build a web server in Julia, and less so in R, but also possible.
This generality calls for factorizing implementation of various tasks into 
library code, as well as gives vast freedom as to which languages
features to employ when building applications. This clearly has downsides.

General-purpose languages take the libraries-based approach to tackle 
data processing (e.g. Pandas in Python, dplyr in R, DataTables in Julia).
Such libraries pose the challenge of learning the APIs with many possible 
caveats and subtle differences between each other.
Another common issue with this approach is tracking changes in
the APIs ones (e.g. functions receiving new parameters, becoming deprecated,
etc.). 

Imperative features of the said languages, especially unchecked mutation
of the global state, may hinder understanding of a script and the grounds
of its validity, which may lead to subtle errors. Bugs in data science 
scripts are a topic of recent studies and can have a profound impact on
the modern society (consider, for example, a recent study of bugs in COVID-19
related software~\cite{bugscovid}).

Both of these issues concerning general-purpose languages when applied to
data analysis tasks can be resolved by the employment of a declarative 
domain-specific language built for this kind of tasks.
As a bonus, a significantly special-purposed DSL shall
allow efficient compilation to a robust application~--- something that
has been an issue with languages like Python and R for decades. 
On the the down side, a declarative interface would lack support
for inherently imperative tasks (e.g. file system manipulation).
This can be resolved by embedding the DSL into a general-purpose 
language.

In this paper, we showcase a commercial implementation of Datalog
embedded in Julia, called Delve, that checks all the above boxes: it is declarative,
has virtually zero amount of API vocabulary needed to apply it,
and it can resort to the efficient JIT compiler of Julia for the tasks
not amenable for declarative processing. Delve's backend is build on solid 
foundations of database systems.

We consider a data analysis application we call Truck Factor.
Two implementations are suggested, using Delve and R. We compare them
along various axes. First, linguistics: how much of code is required,
how many notions a programmer need to learn to build such implementation.
Second, performance and scalability: we consider a spectrum from 
toy loads to the ones not fitting in computer RAM.

\section{Truck Factor}

For a case study, we aimed at a just-above-toy application having to 
do with data analysis. To this end, we picked a task of computing the 
truck factor (also known as bus factor) of a software repository 
roughly following the methodology of~\cite{tf} including the 
degree-of-authorship formula~\cite{doa} as the basis of the metric. 
We did not aim to reproduce results~\cite{tf} 
especially because their dataset of 133 GitHub repositories is insufficient
for our purposes: to make any conclusion performance-wise, we needed
thousands of repositores.

\subsection{Dataset}

We use open source projects hosted on GitHub and using Git as their 
version control history. The history, or metadata, is what analysis
uses as the input with the caveat that it works with CSV-formatted 
tables instead of binary Git-objects. In order to convert the latter 
to the former we use an auxiliary tool called
GhGrabber\footnote{\url{https://github.com/PRL-PRG/ghgrabber}}.

\subsection{Algorithm}

The purpose of the algorithm is to establish the number of a project
(i.e. software repository) contributors who ``authors'' at least half of
the source files. The degree-of-authorship of a file $f$ (represented
by a path $f_p$)
for a developer $d$ (mapped to a GitHub identity $m_d$)
is determined using the following formula~\cite{tf}:
\begin{multline}
\DOA(m_d, f_p) =
  3.293 + \\
  1.098\times \FA(m_d, f_p) +
  0.164\times \DL(m_d, f_p) - \\
  0.321\times \ln(1 + \AC(m_d, f_p)).
\end{multline}
Here, first authorship (FA) is $1$ if $m_d$ originally created $f$, and 
otherwise it is $0$; number of deliveries ($\DL$): number of changes in $f$
made by $m_d$; finally, number of acceptances ($\AC$): number of 
changes in $f$ made by any developer, except $m_d$. The coefficients
are picked up empirically~\cite{doa}.

The high-level structure of the algorithm is: (i) for every project 
source file and every developer touched that file compute the three 
parameters mentioned above ($\FA$, $\DL$, $\AC$) and the final metric ($\DOA$);
(ii) for every file, pick its developer with the maximum $\DOA$ as the file's
author; (iii) sort developers in decreasing order by the number of files 
they author and count how many of the ``top'' developers author at least
half of files.

\subsection{R implementation}

\subsection{Delve implementation}

\section {Discussion}

\section{Conclusion}
